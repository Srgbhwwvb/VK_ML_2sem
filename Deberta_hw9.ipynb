{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9871338,"sourceType":"datasetVersion","datasetId":6059715},{"sourceId":162947,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":138574,"modelId":161216}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import string\n\nimport sys\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nimport torch\n\nimport torch.nn as nn\n\nfrom sklearn.metrics import accuracy_score\n\n\nsys.path.append(\"/kaggle/input/charactertokenizer/transformers/default/1\")\n\nfrom core import CharacterTokenizer\n\n\n\nchars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n\nmodel_max_length = 64\n\ntokenizer = CharacterTokenizer(chars, model_max_length)","metadata":{"id":"5FaCG9ajnS_G","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:08:59.161808Z","iopub.execute_input":"2024-11-11T13:08:59.162677Z","iopub.status.idle":"2024-11-11T13:08:59.175832Z","shell.execute_reply.started":"2024-11-11T13:08:59.162638Z","shell.execute_reply":"2024-11-11T13:08:59.174852Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Задание: обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers. Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n\n\n\n1. Напишите класс для Dataset/Dataloder и разбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\n\n2. Попробуйте обучить одну или несколько из моделей: Bert, Albert, Deberta. Посчитайте метрику Accuracy на train и test. (1 балл). При преодолении порога в Accuracy на test 0.8: (+1 балл), 0.85: (+2 балла), 0.89: (+3 балла).\n\nПример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json","metadata":{"id":"KQkp36rEoScR"}},{"cell_type":"code","source":"df = pd.read_table('/kaggle/input/all-accents/all_accents.tsv', header=None, names = ['word', 'stressed_word'])\n\ndf['stress_idx'] = df['stressed_word'].str.find('^')\n\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.5)\n\n","metadata":{"id":"mRVK6TNAZQFk","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:09:07.623526Z","iopub.execute_input":"2024-11-11T13:09:07.624276Z","iopub.status.idle":"2024-11-11T13:09:12.059081Z","shell.execute_reply.started":"2024-11-11T13:09:07.624237Z","shell.execute_reply":"2024-11-11T13:09:12.058286Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:32.089514Z","iopub.execute_input":"2024-11-11T12:58:32.089840Z","iopub.status.idle":"2024-11-11T12:58:32.108095Z","shell.execute_reply.started":"2024-11-11T12:58:32.089807Z","shell.execute_reply":"2024-11-11T12:58:32.107107Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                   word    stressed_word  stress_idx\n1094902     помышлявшее     помышл^явшее           6\n726284          нанятое         н^анятое           1\n1089385      получающий      получ^ающий           5\n631909        лыжницами       л^ыжницами           1\n1031070    погрузившими    погруз^ившими           6\n...                 ...              ...         ...\n653770     машинальному    машин^альному           5\n1295809     рассовавшие     рассов^авшие           6\n1182500  прифабриваться  приф^абриваться           4\n795115   номинальнейшем  номин^альнейшем           5\n750421   нашинкованного  нашинк^ованного           6\n\n[168053 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>stressed_word</th>\n      <th>stress_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1094902</th>\n      <td>помышлявшее</td>\n      <td>помышл^явшее</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>726284</th>\n      <td>нанятое</td>\n      <td>н^анятое</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1089385</th>\n      <td>получающий</td>\n      <td>получ^ающий</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>631909</th>\n      <td>лыжницами</td>\n      <td>л^ыжницами</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1031070</th>\n      <td>погрузившими</td>\n      <td>погруз^ившими</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>653770</th>\n      <td>машинальному</td>\n      <td>машин^альному</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1295809</th>\n      <td>рассовавшие</td>\n      <td>рассов^авшие</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1182500</th>\n      <td>прифабриваться</td>\n      <td>приф^абриваться</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>795115</th>\n      <td>номинальнейшем</td>\n      <td>номин^альнейшем</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>750421</th>\n      <td>нашинкованного</td>\n      <td>нашинк^ованного</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>168053 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class WordStressDataset(Dataset):\n\n    def __init__(self, df, max_len):\n\n        self.df = df\n\n        self.max_len = max_len\n\n        self.tokenizer = CharacterTokenizer(chars, model_max_length)\n\n\n\n    def __len__(self):\n\n        return len(self.df)\n\n\n\n    def __getitem__(self, idx):\n\n        word = self.df['word'].iloc[idx]\n\n        stress_idx = self.df['stress_idx'].iloc[idx]\n\n\n\n        tokens = self.tokenizer(\n\n            word,\n\n            max_length=self.max_len,\n\n            padding='max_length',\n\n            truncation=True,\n\n            return_tensors='pt'\n\n        )\n\n        labels = torch.zeros((self.max_len), dtype=torch.long)\n\n        if stress_idx > 0:\n\n            labels[stress_idx] = 1\n\n        \n\n        return tokens['input_ids'].flatten(), tokens['attention_mask'].flatten(), labels\n\n\n\n\n\ntrain_dataset = WordStressDataset(train_df, model_max_length)\n\ntest_dataset = WordStressDataset(test_df, model_max_length)\n\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:09:19.327575Z","iopub.execute_input":"2024-11-11T13:09:19.328009Z","iopub.status.idle":"2024-11-11T13:09:19.338799Z","shell.execute_reply.started":"2024-11-11T13:09:19.327973Z","shell.execute_reply":"2024-11-11T13:09:19.337825Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import DebertaV2ForTokenClassification, DebertaV2Config,  AdamW\n\n\n\nconfig = DebertaV2Config(\n\n    architectures=\"DebertaV2ForTokenClassification\",\n\n    model_type=\"deberta-v2\",\n\n    vocab_size=len(tokenizer.get_vocab()),  # Размер словаря (включая специальные токены)\n\n    torch_dtype=\"float32\",\n\n\n    hidden_size=768,  # Размер скрытого слоя\n\n    num_hidden_layers=5,  # Количество скрытых слоёв\n\n    num_attention_heads=12,  # Количество голов внимания\n\n\n    intermediate_size=1024,  # Размер промежуточного слоя\n\n    hidden_act=\"gelu\",  # Функция активации для скрытых слоёв\n\n    hidden_dropout_prob=0.15,  # Вероятность dropout для скрытых слоёв\n\n    attention_probs_dropout_prob=0.15,  # Вероятность dropout для вероятностей внимания\n\n    max_position_embeddings=model_max_length,  # Максимальная длина последовательности\n\n    #type_vocab_size=1,  # Количество типов токенов (для сегментации)\n\n    #initializer_range=0.02,  # Диапазон инициализации весов\n\n    #layer_norm_eps=1e-7,  # Точность нормализации слоёв\n\n    #pad_token_id=0,  # ID токена для заполнения\n\n    #position_embedding_type=\"absolute\",  # Тип позиционного вложения\n\n    #use_cache=True,  # Использовать кеш для ускорения вычислений\n\n\n    num_labels=2,  # Количество классов для задачи классификации (0 - без ударения, 1 - с ударением)\n\n)\n\n\n\nmodel = DebertaV2ForTokenClassification(config)\n\noptimizer = torch.optim.AdamW(model.parameters(),lr = 1e-5,eps = 1e-8)\n\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:09:23.650815Z","iopub.execute_input":"2024-11-11T13:09:23.651653Z","iopub.status.idle":"2024-11-11T13:09:23.946315Z","shell.execute_reply.started":"2024-11-11T13:09:23.651611Z","shell.execute_reply":"2024-11-11T13:09:23.945534Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from tqdm import tqdm\n\n\n\ndef accuracy(preds, labels):    \n\n    return np.all(preds == labels, axis=1).sum() / len(labels)\n\n\n\ndef train(train_dataloader,epoch):\n\n    total_train_loss = 0\n\n    total_train_acc=0\n\n    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n\n    for step, batch in enumerate(pbar):\n\n        b_input_ids = batch[0].to(device)\n\n        b_input_mask = batch[1].to(device)\n\n        b_labels = batch[2].to(device)\n        \n        model.to(device)\n\n        model.zero_grad()\n\n        outputs = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n\n        loss = outputs.loss\n\n        total_train_loss += loss.item()\n\n        logits = torch.argmax(outputs.logits.detach(), dim=2).cpu().numpy()\n\n        label_ids = b_labels.cpu().numpy()\n\n        total_train_acc += accuracy_score(logits, label_ids)\n\n        loss.backward()\n\n        optimizer.step()\n\n\n    avg_train_loss = total_train_loss / len(train_dataloader)\n\n    train_acc = total_train_acc/len(train_dataloader)\n\n    print(\"epoch:\", epoch)\n\n    print(\"Average training loss: \",avg_train_loss)\n\n    print(\"Train Accuracy: \", train_acc)\n\n    return avg_train_loss, train_acc\n\n\n\n\n\ndef validate(val_dataloader, epoch):\n\n    total_eval_accuracy = 0\n\n    total_eval_loss = 0\n\n    pbar = tqdm(val_dataloader, desc=f\"Epoch {epoch}\")\n\n    for batch in pbar:\n\n        b_input_ids = batch[0].to(device)\n\n        b_input_mask = batch[1].to(device)\n\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n\n            outputs = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n\n        loss = outputs.loss\n\n        total_eval_loss += loss.item()\n\n        logits = torch.argmax(outputs.logits.detach(), dim=2).cpu().numpy()\n\n        label_ids = b_labels.cpu().numpy()\n\n        total_eval_accuracy += accuracy_score(logits, label_ids)\n\n\n\n    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n\n    print(\"Validation Accuracy: \",avg_val_accuracy)\n\n    avg_val_loss = total_eval_loss / len(val_dataloader)\n\n    print(\"Validation Loss: \",avg_val_loss)\n\n    \n\n    return avg_val_loss, avg_val_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:13:19.570693Z","iopub.execute_input":"2024-11-11T13:13:19.571595Z","iopub.status.idle":"2024-11-11T13:13:19.586165Z","shell.execute_reply.started":"2024-11-11T13:13:19.571553Z","shell.execute_reply":"2024-11-11T13:13:19.585291Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"for i in range(0, 6):\n\n    model.train()\n\n    avg_train_loss = train(train_dataloader,i)\n\n    model.eval()\n\n    avg_test_loss, avg_test_accuracy = validate(test_dataloader, i)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T13:13:39.743926Z","iopub.execute_input":"2024-11-11T13:13:39.744954Z","iopub.status.idle":"2024-11-11T16:23:46.736415Z","shell.execute_reply.started":"2024-11-11T13:13:39.744910Z","shell.execute_reply":"2024-11-11T16:23:46.735519Z"}},"outputs":[{"name":"stderr","text":"Epoch 0: 100%|██████████| 13130/13130 [21:12<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0\nAverage training loss:  0.020083255425410493\nTrain Accuracy:  0.6137035458353528\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0: 100%|██████████| 13130/13130 [10:28<00:00, 20.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.7549108276212236\nValidation Loss:  0.014407553418375614\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 13130/13130 [21:13<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 1\nAverage training loss:  0.01391884783110514\nTrain Accuracy:  0.7606358832998684\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 13130/13130 [10:30<00:00, 20.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.8142751174155877\nValidation Loss:  0.01156698647416243\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 13130/13130 [21:13<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 2\nAverage training loss:  0.011624277305118285\nTrain Accuracy:  0.8039582323616976\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 13130/13130 [10:29<00:00, 20.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.8447516025641025\nValidation Loss:  0.009477690017057947\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 13130/13130 [21:12<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 3\nAverage training loss:  0.010072207909665423\nTrain Accuracy:  0.8317932995222599\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 13130/13130 [10:32<00:00, 20.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.8708285732419395\nValidation Loss:  0.00817883938805591\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 13130/13130 [21:12<00:00, 10.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 4\nAverage training loss:  0.00894828774066896\nTrain Accuracy:  0.8518173812573564\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 13130/13130 [10:22<00:00, 21.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.8846018976897689\nValidation Loss:  0.007322115608377869\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 13130/13130 [21:10<00:00, 10.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 5\nAverage training loss:  0.008070759062395664\nTrain Accuracy:  0.8665698781416603\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 13130/13130 [10:28<00:00, 20.90it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.8957904925107895\nValidation Loss:  0.006660471626396752\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for i in range(6, 8):\n\n    model.train()\n\n    avg_train_loss = train(train_dataloader,i)\n\n    model.eval()\n\n    avg_test_loss, avg_test_accuracy = validate(test_dataloader, i)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:54.461591Z","iopub.execute_input":"2024-11-11T16:24:54.462445Z","iopub.status.idle":"2024-11-11T17:28:25.549904Z","shell.execute_reply.started":"2024-11-11T16:24:54.462405Z","shell.execute_reply":"2024-11-11T17:28:25.548859Z"}},"outputs":[{"name":"stderr","text":"Epoch 6: 100%|██████████| 13130/13130 [21:18<00:00, 10.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 6\nAverage training loss:  0.0073848336300832524\nTrain Accuracy:  0.8781061760022155\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 13130/13130 [10:28<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.9054927487941101\nValidation Loss:  0.005985709611816411\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 13130/13130 [21:12<00:00, 10.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch: 7\nAverage training loss:  0.006806897448098481\nTrain Accuracy:  0.8878072422626877\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 13130/13130 [10:31<00:00, 20.79it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy:  0.9120438245747651\nValidation Loss:  0.005601048181621679\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"Accuracy on test\", avg_test_accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T17:40:37.810312Z","iopub.execute_input":"2024-11-11T17:40:37.811214Z","iopub.status.idle":"2024-11-11T17:40:37.815929Z","shell.execute_reply.started":"2024-11-11T17:40:37.811172Z","shell.execute_reply":"2024-11-11T17:40:37.815082Z"}},"outputs":[{"name":"stdout","text":"Accuracy on test 0.9120438245747651\n","output_type":"stream"}],"execution_count":22}]}